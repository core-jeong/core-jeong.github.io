---
layout: post
current: post
cover:  assets/images/AI.jpg
navigation: True
title: '[모두를 위한 딥러닝 정리] ML lec 04 - Multivariable linear regression'
date: 2018-08-08 23:00:00
tags: [DEEP LEARNING]
class: post-template
subclass: 'post tag-machine-learning'
author: core-jeong
---

Multivariable Linear Regression
===
## 복습
### Lenear Regression을 설계하기 위한 3가지
#### 1. Hypothesis  
$${H(x)} = {Wx + b}$$

#### 2. Cost(loss) Function
$$cost(W,b) = {1 \over m}\sum_{i=1}^m (H(x_{i})-y_{i})^2$$

#### 3. Gradient Descent Algorithm

***
## Multivariable Linear Regression
>  여러개의 인풋(X1, X2, X3)이 주어질 때 결과(Y)를 예측할 수 있을까?

| X1(quiz1) | x2(quiz2) | x3(midterm1) | Y(final) |
|:--------- | :--------- | :---------| :---------|
| 73 | 80 | 75 | 152 |
| 93 | 88 | 93 | 185 |
| 89 | 91 | 90 | 180 |
| 96 | 98 | 100 | 196 |
| 73 | 66 | 70 | 142 |

### 1. Hypothesis
$${H(x_{1},x_{2},x_{3},...,x_{n})} = {w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{3} + ... + w_{n}x_{n} + b}$$
### 2. Cost(loss) Function
$$cost(W,b) = {1 \over m}\sum_{i=1}^m (H(x_{1i},x_{2i},x_{3i},...,x_{ni})-y_{i})^2$$
#### 행렬곱(Matrix Multiplication)
* Hypothesis가 복잡해지는데 이는 행렬곱으로 해결가능
$${\begin{pmatrix} x_{1} & x_{2} & x_{3} \end{pmatrix} \cdot \begin{pmatrix} w_{1} \\ w_{2} \\ w_{3} \end{pmatrix}} = {\begin{pmatrix} x_{1}w_{1} + x_{2}w_{2} + x_{3}w_{3} \end{pmatrix}}$$
$${H(x)} = {XW}$$

* Matrix의 장점 중 하나는 여러개의 인스턴스를 한번에 표현할 수 있다

$${\begin{pmatrix} x_{11} & x_{12} & x_{13} \\ x_{21} & x_{22} & x_{23} \\ x_{31} & x_{32} & x_{33}\\ x_{41} & x_{42} & x_{43} \\ x_{51} & x_{52} & x_{53} \end{pmatrix} \cdot \begin{pmatrix} w_{1} \\ w_{2} \\ w_{3} \end{pmatrix}} = {\begin{pmatrix} x_{11}w_{1}+x_{12}w_{2}+x_{13}w_{3} \\ x_{21}w_{1}+x_{22}w_{2}+x_{23}w_{3} \\ x_{31}w_{1}+x_{32}w_{2}+x_{33}w_{3}\\ x_{41}w_{1}+x_{42}w_{2}+x_{43}w_{3} \\ x_{51}w_{1}+x_{52}w_{2}+x_{53}w_{3} \end{pmatrix}}$$

$$[5,3] * [3,1] = [5,1]$$

* 3은 변수의 갯수(주어짐)
* 5는 인스턴스의 개수(주어짐)
* 1은 결과의 개수
***
> 본 포스트는 Youtube Sung Kim님의 강의를 정리한 내용입니다. 문제가 될 경우 삭제하겠습니다.
