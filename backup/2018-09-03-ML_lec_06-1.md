---
layout: post
current: post
cover:  assets/images/AI.jpg
navigation: True
title: '[모두를 위한 딥러닝 정리] ML lec 06-1 - Softmax Regression: 기본 개념 소개'
date: 2018-09-03 23:00:00
tags: [DEEP LEARNING]
class: post-template
subclass: 'post tag-machine-learning'
author: core-jeong
---

Softmax classification: Multinomial classification
===

## Logistic Classification의 목적
* 두 클래스를 구별하는 선을 만듬

## Multinomial Classification의 예
| X1(hours) | x2(attendance) | y(grade) |
|:--------- | :--------- | :---------|
| 10 | 5 | A |
| 9 | 5 | A |
| 3 | 2 | B |
| 2 | 4 | B |
| 11 | 1 | C |
### Hypothesis
1. C인지 아닌지 판별하는 Classification을 만듬
$${\begin{pmatrix} w_{A1} & w_{A2} & w_{A3}\end{pmatrix} \cdot \begin{pmatrix} x_{1} \\ x_{2} \\ x_{3} \end{pmatrix}} = {\begin{pmatrix} w_{A1}x_{1}+w_{A2}x_{2}+w_{A3}x_{3}\end{pmatrix}}$$
2. B인지 아닌지 판별하는 Classification을 만듬
$${\begin{pmatrix} w_{B1} & w_{B2} & w_{B3}\end{pmatrix} \cdot \begin{pmatrix} x_{1} \\ x_{2} \\ x_{3} \end{pmatrix}} = {\begin{pmatrix} w_{B1}x_{1}+w_{B2}x_{2}+w_{B3}x_{3}\end{pmatrix}}$$
3. A인지 아닌지 판별하는 Classification을 만듬
$${\begin{pmatrix} w_{C1} & w_{C2} & w_{C3}\end{pmatrix} \cdot \begin{pmatrix} x_{1} \\ x_{2} \\ x_{3} \end{pmatrix}} = {\begin{pmatrix} w_{C1}x_{1}+w_{C2}x_{2}+w_{C3}x_{3}\end{pmatrix}}$$
#### 요약
$${\begin{pmatrix} w_{A1} & w_{A2} & w_{A3} \\ w_{B1} & w_{B2} & w_{B3} \\ w_{C1} & w_{C2} & w_{C3}\end{pmatrix} \cdot \begin{pmatrix} x_{1} \\ x_{2} \\ x_{3} \end{pmatrix}} = {\begin{pmatrix} w_{A1}x_{1}+w_{A2}x_{2}+w_{A3}x_{3} \\ w_{B1}x_{1}+w_{B2}x_{2}+w_{B3}x_{3} \\ w_{C1}x_{1}+w_{C2}x_{2}+w_{C3}x_{3}\end{pmatrix}} = {\begin{pmatrix} \bar{y_{_A}} \\ \bar{y_{_B}} \\ \bar{y_{_C}} \end{pmatrix}} = {\begin{pmatrix} 2.0 \\ 1.0 \\ 0.1 \end{pmatrix}}$$

#### Softmax란?
* 결과 백터를 0과 1사이의 값 즉 확률값으로 만듬
$$S(y_i) = {e^{y_i} \over \sum_{j}{e^{y_j}}}$$
${\begin{pmatrix} 2.0 \\ 1.0 \\ 0.1 \end{pmatrix}} \cdot S(y_i) = {\begin{pmatrix} 0.7 \\ 0.2 \\ 0.1 \end{pmatrix}}$
#### One-Hot Encoding기법
* 값이 가장 큰 값은 1로 나머지는 0으로 바꿈
${\begin{pmatrix} 0.7 \\ 0.2 \\ 0.1 \end{pmatrix}} => {\begin{pmatrix} 0.1 \\ 0.0 \\ 0.0 \end{pmatrix}}$
***
### Cost Function
#### Cross Entropy
* 정답($L$, $Y$)과 예측값($S$, $\bar{Y}$) 사이의 차를 계산
$$D(S,L) = -\sum_{i}L_ilog(S_i)$$
* $L={\begin{pmatrix} 0 \\ 1\end{pmatrix}}$, $\bar{Y}={\begin{pmatrix} 0 \\ 1\end{pmatrix}}$ 일때,
$D(S,L) =
{\begin{pmatrix} 0 \\ 1\end{pmatrix}}\odot -log{\begin{pmatrix} 0 \\ 1\end{pmatrix}} =
{\begin{pmatrix} 0 \\ 1\end{pmatrix}}\odot {\begin{pmatrix} \infty \\ 0\end{pmatrix}} =
{\begin{pmatrix} 0 \\ 0\end{pmatrix}} = 0$

* $L={\begin{pmatrix} 0 \\ 1\end{pmatrix}}$, $\bar{Y}={\begin{pmatrix} 1 \\ 0\end{pmatrix}}$ 일때,
$D(S,L) =
{\begin{pmatrix} 0 \\ 1\end{pmatrix}}\odot -log{\begin{pmatrix} 1 \\ 0\end{pmatrix}}  =
{\begin{pmatrix} 0 \\ 1\end{pmatrix}}\odot {\begin{pmatrix} 0 \\ \infty\end{pmatrix}} =  {\begin{pmatrix} 0 \\ \infty\end{pmatrix}} = \infty$ (예측이 틀렸을때, 무한히 큰 값을 준다)

* $L={\begin{pmatrix} 1 \\ 0\end{pmatrix}}$, $\bar{Y}={\begin{pmatrix} 0 \\ 1\end{pmatrix}}$ 일때,
$D(S,L) =
{\begin{pmatrix} 1 \\ 0\end{pmatrix}}\odot {\begin{pmatrix} \infty \\ 0\end{pmatrix}}=
{\begin{pmatrix} \infty \\ 0\end{pmatrix}} = \infty$

* $L={\begin{pmatrix} 1 \\ 0\end{pmatrix}}$, $\bar{Y}={\begin{pmatrix} 1 \\ 0\end{pmatrix}}$ 일때,
$D(S,L) = {\begin{pmatrix} 1 \\ 0\end{pmatrix}}\odot {\begin{pmatrix} 0 \\ \infty\end{pmatrix}}= {\begin{pmatrix} 0 \\ 0\end{pmatrix}} = 0$
#### Cost Function
$$Cost(W) = {1 \over N}\sum_{i} {D(S(Wx_i+b),L_i)}$$
***
> 본 포스트는 Youtube Sung Kim님의 강의를 정리한 내용입니다. 문제가 될 경우 삭제하겠습니다.
