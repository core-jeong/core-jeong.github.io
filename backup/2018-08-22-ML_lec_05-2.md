---
layout: post
current: post
cover:  assets/images/AI.jpg
navigation: True
title: '[모두를 위한 딥러닝 정리] ML lec 05 - Logistic (regression) classification: cost function & gradient decent'
date: 2018-08-22 22:00:00
tags: [DEEP LEARNING]
class: post-template
subclass: 'post tag-machine-learning'
author: core-jeong
---

Logistic (regression) classification: cost function & gradient decent
===
## Logistic Hypothesis
$${H(X)} = {1 \over (1+e^{-W^{T}X})}$$
* 기존과 동일한 cost function을 그리면 구불구불한 그래프가 그려짐
* gradient decent algorithm을 돌리면 시작점에 따라 다른값이 나올 수 있음(=global minimum을 찾을 수 없음)
# 구불한 그래프 넣어야됌!!!

## New cost function for logistic
$${cost(W)} = {1 \over m}{\sum} c(H(x),y)$$
$$c(H(x),y) = \begin{cases}
-\log(H(x)), & :y { = 1} \\
-\log(1-H(x)), & :y { = 0}
\end{cases}$$
* y=1이고 H(x) = 1일때, cost(1)=0
* y=1이고 H(x) = 0일때, cost(0)=inf
# 왼쪽포물선 그래프 넣어야됌!!!
* y=0이고 H(x) = 0일때, cost(0)=0
* y=0이고 H(x) = 1일때, cost(1)=inf
# 오른쪽포물선 그래프 넣어야됌!!!
### Tensorflow에서 사용하기위해 간략히 수정한식
$$c(H(x),y) = -ylog(H(x))-(1-y)log(1-H(x))$$
$${cost(W)} = -{1 \over m}{\sum}ylog(H(x))+(1-y)log(1-H(x))$$
## Minimize cost: Gradient decent algorithm
$$W := W-\alpha{\delta \over \delta W}{cost(W)}$$
* 미분이 복잡하기 때문에 컴퓨터에게 맡김

***
> 본 포스트는 Youtube Sung Kim님의 강의를 정리한 내용입니다. 문제가 될 경우 삭제하겠습니다.
