---
layout: post
current: post
cover:  assets/images/AI.jpg
navigation: True
title: '[모두를 위한 딥러닝 정리] ML lab 04-1 - Multi-variable linear regression'
date: 2018-08-13 23:30:00
tags: [DEEP LEARNING]
class: post-template
subclass: 'post tag-machine-learning'
author: core-jeong
---

Multi-variable Linear Regression
===
## Hypothesis using Matrix

| X1 | x2 | x3 | Y |
|:--------- | :--------- | :---------| :---------|
| 73 | 80 | 75 | 152 |
| 93 | 88 | 93 | 185 |
| 89 | 91 | 90 | 180 |
| 96 | 98 | 100 | 196 |
| 73 | 66 | 70 | 142 |
[Test Scores for General Psychology]

$${H(x_{1},x_{2},x_{3})} = {w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{3}}$$
* X의 수가 많을수록 정확도는 높아짐

### Matrix를 적용하지 않은 소스코드
```python
import tensorflow as tf

x1_data = [73., 93., 89., 96., 73.]
x2_data = [80., 88., 91., 98., 66.]
x3_data = [75., 93., 90., 100., 70.]
y_data = [152., 185., 180., 196., 142.]

# placeholders for a tensor that will be always fed.
x1 = tf.placeholder(tf.float32)
x2 = tf.placeholder(tf.float32)
x3 = tf.placeholder(tf.float32)

Y = tf.placeholder(tf.float32)

w1 = tf.Variable(tf.random_normal([1]), name='weight1')
w2 = tf.Variable(tf.random_normal([1]), name='weight2')
w3 = tf.Variable(tf.random_normal([1]), name='weight3')
b = tf.Variable(tf.random_normal([1]), name='bias')

hypothesis = x1 * w1 + x2 * w2 + x3 * w3 + b

# cost/loss function
cost = tf.reduce_mean(tf.square(hypothesis - Y))
# Minimize. Need a very small learning rate for this data set
optimizer = tf.train.ProximalGradientDescentOptimizer(learning_rate=1e-5)
train = optimizer.minimize(cost)

# Launch the graph in a session.
sess = tf.Session()
# Initializes global variables in the graph
sess.run(tf.global_variables_initializer())
for step in range(2001):
    cost_val, hy_val, _ = sess.run([cost, hypothesis, train],
                                   feed_dict={x1: x1_data, x2: x2_data, x3: x3_data, Y: y_data})
    if step % 10 == 0:
        print(step, "Cost: ", cost_val, "\nPrediction:\n", hy_val)
```
* X의 수가 많아지면 코드가 복잡해지는 단점이 있다

### 실행결과
```
0 Cost:  18049.95
Prediction:
 [40.55964  35.8033   41.96189  45.443325 24.316929]
10 Cost:  37.112213
Prediction:
 [159.26222 178.51909 182.55927 198.55263 133.18222]
20 Cost:  36.750572
Prediction:
 [159.59816 178.96687 182.9776  199.01048 133.53282]
 .
 .
 .
 1980 Cost:  13.043097
Prediction:
 [156.06577 181.39651 181.90381 198.1748  136.77069]
1990 Cost:  12.975325
Prediction:
 [156.05215 181.40588 181.89967 198.17154 136.78323]
2000 Cost:  12.907962
Prediction:
 [156.0386  181.41522 181.89557 198.16827 136.79575]

Process finished with exit code 0
```
* 예측값이 실제 Y와 비슷해짐

### Matrix
$${\begin{pmatrix} x_{1} & x_{2} & x_{3} \end{pmatrix} \cdot \begin{pmatrix} w_{1} \\ w_{2} \\ w_{3} \end{pmatrix}} = {\begin{pmatrix} x_{1}w_{1} + x_{2}w_{2} + x_{3}w_{3} \end{pmatrix}}$$
$${H(x)} = {XW}$$

#### Matrix를 적용한 소스코드
```python
import tensorflow as tf

x_data = [[73., 80., 75.], [93., 88., 93.], [89., 91., 90.,], [96., 98., 100.], [73., 66., 70.]]
y_data = [[152.], [185.], [180.], [196.], [142.]]

# placeholders for a tensor that will be always fed.
X = tf.placeholder(tf.float32, shape=[None, 3])
Y = tf.placeholder(tf.float32, shape=[None, 1])

W = tf.Variable(tf.random_normal([3, 1]), name='weight')
b = tf.Variable(tf.random_normal([1]), name='bias')

# Hypothesis
hypothesis = tf.matmul(X, W) + b

# Simplified cost/loss function
cost = tf.reduce_mean(tf.square(hypothesis - Y))
# Minimize
optimizer = tf.train.ProximalGradientDescentOptimizer(learning_rate=1e-5)
train = optimizer.minimize(cost)

# Launch the graph in a session.
sess = tf.Session()
# Initializes global variables in the graph
sess.run(tf.global_variables_initializer())
for step in range(2001):
    cost_val, hy_val, _ = sess.run([cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})
    if step % 10 == 0:
        print(step, "Cost: ", cost_val, "\nPrediction:\n", hy_val)
```
* 데이터가 많아도 쉽게 구할 수 있다

***
> 본 포스트는 Youtube Sung Kim님의 강의를 정리한 내용입니다. 문제가 될 경우 삭제하겠습니다.
