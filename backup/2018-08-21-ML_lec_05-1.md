---
layout: post
current: post
cover:  assets/images/AI.jpg
navigation: True
title: '[모두를 위한 딥러닝 정리] ML lec 05 - Logistic (regression) classification'
date: 2018-08-08 23:00:00
tags: [DEEP LEARNING]
class: post-template
subclass: 'post tag-machine-learning'
author: core-jeong
---

Logistic (regression) classification
===
## 복습
### Lenear Regression
| X1(hours) | x2(attendance) | y(score) |
|:--------- | :--------- | :---------|
| 10 | 5 | 90 |
| 9 | 5 | 80 |
| 3 | 2 | 50 |
| 2 | 4 | 60 |
| 11 | 1 | 40 |
#### 1. Hypothesis  
$${H(x)} = {Wx}$$

#### 2. Cost
$$cost(W) = {1 \over m}\sum (Wx-y)^2$$

#### 3. Gradient Descent
$$W := W-\alpha{\delta \over \delta W}{cost(W)}$$
***
## Classification
* Regression은 어떤 값을 예측하는 반면, 변수 중에서 하나의 카테고리를 고르는 것
### 예시
#### 1. 0/1 Encoding
  * 스팸(1), 햄(0) 메일 구별
  * SNS 타임라인 보이기(1), 숨김(0) 시스템
  * 신용카드 사용(0), 분실(1) 판별

#### 2. 뇌 종양 판별
#### 3. 주식 예측
### Binary Classification
* 선을 그려 그것보다 위에 있으면 1, 아래에 있으면 0
* 직선을 그리면 오분류 되거나 소수의 튀는 값 때문에 선이 잘못그려질 수 있음
* 곡선형의 0, 1사이의 시그모이드(=로지스틱)함수 사용
$${g(z)} = {1 \over (1+e^{-z})}$$
### Logistic Hypothesis
$$g(z) = H(X) = Wx$$
$${H(X)} = {1 \over (1+e^{-W^{T}X})}$$
* (W^T)X는 벡터의 형태에 따라 Wx로 사용가능

***
> 본 포스트는 Youtube Sung Kim님의 강의를 정리한 내용입니다. 문제가 될 경우 삭제하겠습니다.
