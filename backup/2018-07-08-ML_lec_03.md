---
layout: post
current: post
cover:  assets/images/AI.jpg
navigation: True
title: '[모두를 위한 딥러닝 정리] ML lec 03 - How to minimize cost'
date: 2018-07-08 18:00:00
tags: [DEEP LEARNING]
class: post-template
subclass: 'post tag-machine-learning'
author: core-jeong
---

How to minimize cost
===

## Simplified Hypothesis
* 기존 Hypothesis에서 y절편 b 제거
$${H(x)} = {Wx}$$
$${cost(W)} = {1 \over m}\sum_{i=1}^m (Wx_{i}-y_{i})^2$$
* 이 cost를 최소화하는 것이 과제
## cost는 어떻게 생겼을까?
| X | Y |
|:--------- | :--------- |
| 1 | 1 |
| 2 | 2 |
| 3 | 3 |
* W=1일때 cost(W)는?
$$cost(W) = {1 \over 3}((1*1 - 1)^2 + (1*2 - 2)^2 + (1*3 - 3)^2) = 0$$
* W=0일때 cost(W)는?
$$cost(W) = {1 \over 3}((0*1 - 1)^2 + (0*2 - 2)^2 + (0*3 - 3)^2) = 4.67$$
* W=2일때 cost(W)는?
$$cost(W) = {1 \over 3}((2*1 - 1)^2 + (2*2 - 2)^2 + (2*3 - 3)^2) = 4.67$$

* 그래프를 그리면 포물선 모양이 됨
$${cost(W)} = {1 \over m}\sum_{i=1}^m (Wx_{i}-y_{i})^2$$

![result](../assets/images/ML_lec_03/01.png)

## Gradientt Descent Algorithm
* 경사를 따라 내려가면 cost를 최소화 하는 W를 찾을 수 있음

### How it works?
1. 아무점에서 시작
2. W를 조금 수정
3. 경사도를 계산(미분)
4. 2.반복

### Formal definition
* 미분을 쉽게 하기 위해 2를 나눔
$${cost(W)} = {1 \over 2m}\sum_{i=1}^m (Wx_{i}-y_{i})^2$$

* W값에 learning rate(작은 값)와 cost함수를 미분한값(기울기)를 곱한것을 빼서 W를 수정
$$W := W-\alpha{\delta \over \delta W}{cost(W)}$$

  * 기울기가 양수면 왼쪽으로 이동하고 음수이면 오른쪽으로 이동함

#### 미분결과
$$W := W-\alpha{\delta \over \delta W}{1 \over 2m}\sum_{i=1}^m (Wx_{i}-y_{i})^2$$
$$W := W-\alpha{1 \over 2m}\sum_{i=1}^m 2(Wx_{i}-y_{i})x_{i}$$
$$W := W-\alpha{1 \over m}\sum_{i=1}^m(Wx_{i}-y_{i})x_{i}$$

### Convex Function
 * cost함수를 설계할때 어떤점에서 시작해도 기울기가 0인값을 찾아야함

***
> 본 포스트는 Youtube Sung Kim님의 강의를 정리한 내용입니다. 문제가 될 경우 삭제하겠습니다.
